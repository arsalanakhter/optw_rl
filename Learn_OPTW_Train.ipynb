{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "When we take a dot product of query and key values, the variance of the result of the dot product may be scaled by $d$, which is the dimension of the key/query vectors. To ensure that the variance of the dot product still remains the same regardless of vector length, the scaled dot-product attention scoring function is used.\n",
    "\n",
    "Assuming $\\mathbf{Q} \\in \\mathbb{R}^{n \\times d}$, keys $\\mathbf{K}\\in \\mathbb{R}^{m \\times d}$, values $\\mathbf{V}\\in \\mathbb{R}^{m \\times v}$ we do\n",
    "\n",
    "$$\\text{softmax} \\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right)\\mathbf{V} \\quad \\in \\mathbb{R}^{n\\times v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the paper suggests, we apply masking when we do not want certain nodes to be considered for training. It makes sense to only consider those nodes which are part of a feasible sequence. A feasible sequence is basically [current node, $v_i$, $v_j$, $v_n$ ]. In other words, this becomes a one-step look-ahead search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Vanilla Transformers, we\n",
    "\n",
    "1. Take the input vectors $\\{e_1, e_2, ..., e_n\\}$\n",
    "2. Compute Keys and Values using Linear Transformations\n",
    "\n",
    "$$k_i = W_k e_i$$\n",
    "$$v_i = W_v e_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then determine a similarity score between each $e_i$ and $k_j$, using scaled dot-product attention.\n",
    "\n",
    "$$u_{ij} = \\frac{e_i^T W_q^Tk_j}{\\sqrt{d_k}} \\quad \\forall i,j \\in \\{1, ..., n\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = np.sqrt(d_k)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        # q: [b_size x len_q x d_k]\n",
    "        # k: [b_size x len_k x d_k]\n",
    "        # v: [b_size x len_v x d_v] note: (len_k == len_v)\n",
    "        # Batch-Matrix Multiplication\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) / self.scale_factor  # attn: [b_size x len_q x len_k]\n",
    "        if attn_mask is not None:\n",
    "        #    assert attn_mask.size() == attn.size()\n",
    "            attn.data.masked_fill_(attn_mask==0, -1e32)\n",
    "\n",
    "        attn = self.softmax(attn )\n",
    "        outputs = torch.bmm(attn, v) # outputs: [b_size x len_q x d_v]\n",
    "        return outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x # inputs: [b_size x len_q x d_model]\n",
    "        outputs = self.w_2(F.relu(self.w_1(x)))\n",
    "        return self.layer_norm(residual + outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(_MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.w_q = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_k))\n",
    "        self.w_k = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_k))\n",
    "        self.w_v = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_v))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None, is_adj=True):\n",
    "        (d_k, d_v, d_model, n_heads) = (self.d_k, self.d_v, self.d_model, self.n_heads)\n",
    "        b_size = k.size(0)\n",
    "\n",
    "        q_s = q.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_q x d_model]\n",
    "        k_s = k.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_k x d_model]\n",
    "        v_s = v.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_v x d_model]\n",
    "\n",
    "        q_s = torch.bmm(q_s, self.w_q).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_q x d_k]\n",
    "        k_s = torch.bmm(k_s, self.w_k).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_k x d_k]\n",
    "        v_s = torch.bmm(v_s, self.w_v).view(b_size * n_heads, -1, d_v)  # [b_size * n_heads x len_v x d_v]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if is_adj:\n",
    "                outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_heads, 1, 1))\n",
    "            else:\n",
    "                outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.unsqueeze(1).repeat(n_heads, 1, 1))\n",
    "        else:\n",
    "            outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=None)\n",
    "\n",
    "        return torch.split(outputs, b_size, dim=0), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.attention = _MultiHeadAttention(d_model, n_heads)\n",
    "        self.proj = nn.Linear(n_heads * self.d_k, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask = None, is_adj = True):\n",
    "        # q: [b_size x len_q x d_model]\n",
    "        # k: [b_size x len_k x d_model]\n",
    "        # v: [b_size x len_v x d_model] note (len_k == len_v)\n",
    "        residual = q\n",
    "        # outputs: a list of tensors of shape [b_size x len_q x d_v] (length: n_heads)\n",
    "        outputs, attn = self.attention(q, k, v, attn_mask=attn_mask, is_adj=is_adj)\n",
    "        # concatenate 'n_heads' multi-head attentions\n",
    "        outputs = torch.cat(outputs, dim=-1)\n",
    "        # project back to residual size, result_size = [b_size x len_q x d_model]\n",
    "        outputs = self.proj(outputs)\n",
    "\n",
    "        return self.layer_norm(residual + outputs), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inp, rec_enc_inp, self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inp, rec_enc_inp, enc_inp, attn_mask=self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features_dim, dfeatures_dim, hidden_size, args):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        n_heads = args.n_heads # number of heads\n",
    "        d_ff = args.ff_dim # feed_forward_hidden\n",
    "        n_layers = args.n_layers # number of Layers\n",
    "\n",
    "        self.L1 = nn.Linear(features_dim, hidden_size//2) # for static features\n",
    "        self.L2 = nn.Linear(dfeatures_dim, hidden_size//2) # for dynamic features\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_size, d_ff, n_heads) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, emb_inp, rec_inp, mask, dummy_arg):\n",
    "        for layer in self.layers:\n",
    "            emb_inp, _ = layer(emb_inp, rec_inp, mask)\n",
    "\n",
    "        return emb_inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Parameter(torch.zeros((hidden_size, 1), requires_grad=True))\n",
    "\n",
    "        self.first_h_0 = nn.Parameter(torch.FloatTensor(1, hidden_size), requires_grad=True)\n",
    "        self.first_h_0.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))\n",
    "\n",
    "        self.c0 = nn.Parameter(torch.FloatTensor( 1, hidden_size),requires_grad=True)\n",
    "        self.c0.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))\n",
    "\n",
    "        self.hidden_0 = (self.first_h_0, self.c0)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, enc_outputs, mask):\n",
    "        hidden = self.lstm(input, hidden)\n",
    "        w1e = self.W1(enc_outputs)\n",
    "        w2h = self.W2(hidden[0]).unsqueeze(1)\n",
    "        u = torch.tanh(w1e + w2h)\n",
    "        a = u.matmul(self.V)\n",
    "        a = 10*torch.tanh(a).squeeze(2)\n",
    "\n",
    "        policy = F.softmax(a + mask.float().log(), dim=1)\n",
    "\n",
    "        return policy, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecPointerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, features_dim, dfeatures_dim, hidden_dim, args):\n",
    "        super(RecPointerNetwork, self).__init__()\n",
    "\n",
    "        self.features_dim = features_dim\n",
    "        self.dfeatures_dim = dfeatures_dim\n",
    "        self.use_checkpoint = args.use_checkpoint\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "        self.encoder = Encoder(features_dim, dfeatures_dim, hidden_dim, args)\n",
    "        # see https://discuss.pytorch.org/t/checkpoint-with-no-grad-requiring-inputs-problem/19117/11\n",
    "        self.dummy_tensor = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def _load_model_weights(self, path_string, device):\n",
    "        self.load_state_dict(torch.load(path_string, map_location=device))\n",
    "\n",
    "\n",
    "    def forward(self, enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step=False):\n",
    "        policy, dec_hidden, enc_outputs = self._one_step(enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step)\n",
    "        return policy, dec_hidden, enc_outputs\n",
    "\n",
    "    def _one_step(self, enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step):\n",
    "        if self.use_checkpoint:\n",
    "            enc_outputs = checkpoint(self.encoder, enc_inputs, enc_hidden, adj_mask, self.dummy_tensor)\n",
    "        else:\n",
    "            enc_outputs = self.encoder(enc_inputs, enc_hidden, adj_mask, self.dummy_tensor)\n",
    "\n",
    "        if first_step:\n",
    "            return  None, None, enc_outputs\n",
    "        else:\n",
    "            policy, dec_hidden = self.decoder(dec_input, dec_hidden, enc_outputs, mask)\n",
    "            return policy, dec_hidden, enc_outputs\n",
    "\n",
    "    def sta_emb(self, sta_inp):\n",
    "        return torch.tanh(self.encoder.L1(sta_inp))\n",
    "\n",
    "    def dyn_emb(self, dyn_inp):\n",
    "        return torch.tanh(self.encoder.L2(dyn_inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py in the original code\n",
    "cf = dict(\n",
    "        BENCHMARK_INSTANCES_PATH = './data/benchmark/',\n",
    "        GENERATED_INSTANCES_PATH = './data/generated/',\n",
    "        RESULTS_PATH = './results'\n",
    ")\n",
    "cf = DotMap(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_config.py in the original code\n",
    "pcf = DotMap(dict(\n",
    "# Indices of instance data\n",
    "X_COORDINATE_IDX = 0,\n",
    "Y_COORDINATE_IDX = 1,\n",
    "VIS_DURATION_TIME_IDX = 2,\n",
    "OPENING_TIME_WINDOW_IDX = 3,\n",
    "CLOSING_TIME_WINDOW_IDX = 4,\n",
    "REWARD_IDX = 5,\n",
    "ARRIVAL_TIME_IDX = 6,\n",
    "\n",
    "# For generating instances\n",
    "SAMP_DAY_FRAC_INF = 4/24,\n",
    "UB_T_INIT_FRAC = 15/24,\n",
    "LB_T_MAX_FRAC = 12/24,\n",
    "CORR_SCORE_STD = 10,\n",
    "\n",
    "MULTIPLE_SCORE = 1.1,\n",
    "\n",
    "X_MAX = 100. # max square length (X_MAX)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_n_vert_1(instance_data, instance_type):\n",
    "    N_VERT_ROW = 0\n",
    "\n",
    "    if instance_type=='Gavalas':\n",
    "        N_VERT_COL = 3\n",
    "        DATA_INIT_ROW = 1\n",
    "    else:\n",
    "        N_VERT_COL = 2\n",
    "        DATA_INIT_ROW = 2\n",
    "\n",
    "    n_vert = instance_data[N_VERT_ROW][N_VERT_COL]\n",
    "    count_vert = len(instance_data)-(DATA_INIT_ROW+1)\n",
    "\n",
    "    assert count_vert==n_vert, 'number of vertices doesnt match number of data rows'\n",
    "\n",
    "\n",
    "def test_n_vert_2(instance_data, instance_type):\n",
    "    N_VERT_ROW = 0\n",
    "    if instance_type=='Gavalas':\n",
    "        N_VERT_COL = 3\n",
    "    else:\n",
    "        N_VERT_COL = 2\n",
    "    COLUMN_NAMES = ['vertex number', 'x coordinate', 'y coordinate',\n",
    "                    'service duration or visiting time', 'profit of the location',\n",
    "                    'not relevant 1', 'not relevant 2', 'not relevant 3',\n",
    "                    'opening of time window', 'closing of time window']\n",
    "    COLUMN_NAMES = [s.replace(' ', '_') for s in COLUMN_NAMES]\n",
    "\n",
    "    VERTEX_NUMBER_COL = [i for i,n in enumerate(COLUMN_NAMES) if n=='vertex_number'][0]\n",
    "    n_vert = instance_data[N_VERT_ROW][N_VERT_COL]\n",
    "    last_vert_number = instance_data[-1][VERTEX_NUMBER_COL]\n",
    "\n",
    "    assert last_vert_number==n_vert, 'number of vertices doesnt match vertice count of last row'\n",
    "\n",
    "\n",
    "def test_n_vert_3(instance_data, instance_type):\n",
    "    if instance_type=='Gavalas':\n",
    "        N_DAYS_INDEX = 1\n",
    "        n_days = int(np.array(instance_data[0])[N_DAYS_INDEX])\n",
    "        assert n_days==1, 'not a single tour/1 day instance'\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instance_data_Gavalas(instance_data):\n",
    "    \"\"\"parse instance data into dataframe\"\"\"\n",
    "\n",
    "    # get start date\n",
    "    N_DAYS_INDEX = 1\n",
    "    START_DAY_INDEX = 2\n",
    "    M = instance_data[0][N_DAYS_INDEX]\n",
    "    SD =  int(instance_data[0][START_DAY_INDEX])\n",
    "\n",
    "    OPENING_TIME_WINDOW_ABBREV_KEY = 'O'\n",
    "    CLOSING_TIME_WINDOW_ABBREV_KEY = 'C'\n",
    "    TOTAL_TIME_KEY = 'Total Time'\n",
    "    COLUMN_NAMES_ABBREV = ['i', 'x', 'y', 'd', 'S', 't',\n",
    "                           'open_0', 'close_0', 'open_1', 'close_1',\n",
    "                           'open_2', 'close_2', 'open_3', 'close_3',\n",
    "                           'open_4', 'close_4', 'open_5', 'close_5',\n",
    "                           'open_6', 'close_6', 'b']\n",
    "\n",
    "    df = pd.DataFrame(instance_data[2:], columns=COLUMN_NAMES_ABBREV)\n",
    "\n",
    "    df_ = df[['i', 'x', 'y', 'd', 'S', 't']+[c for c in df.columns if c[-1]==str(SD)]]\n",
    "    columns = ['i', 'x', 'y', 'd', 'S', 't', OPENING_TIME_WINDOW_ABBREV_KEY, CLOSING_TIME_WINDOW_ABBREV_KEY]\n",
    "    df_.columns=columns\n",
    "\n",
    "    aux = pd.DataFrame([instance_data[1]], columns = ['i', 'x', 'y', 'd', 'S', 'O', 'C'])\n",
    "    df = pd.concat([aux, df_], axis=0, sort=True).reset_index(drop=True)\n",
    "\n",
    "    #add total time\n",
    "    df[TOTAL_TIME_KEY] = 0\n",
    "    df[TOTAL_TIME_KEY] = df.loc[0][CLOSING_TIME_WINDOW_ABBREV_KEY]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_instance_data(instance_name, path):\n",
    "\n",
    "    \"\"\"reads instance data\"\"\"\n",
    "    PATH_TO_BENCHMARK_INSTANCES = path\n",
    "\n",
    "    benchmark_file = '{path_to_benchmark_instances}/{instance}.txt' \\\n",
    "                     .format(path_to_benchmark_instances=PATH_TO_BENCHMARK_INSTANCES,\n",
    "                             instance=instance_name)\n",
    "\n",
    "    dfile = open(benchmark_file)\n",
    "    data = [[float(x) for x in line.split()] for line in dfile]\n",
    "    dfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(instance_df, instance_type):\n",
    "    \"\"\"\n",
    "    Distances between locations were rounded down to the first decimal\n",
    "    for the Solomon instances and to the second decimal for the instances of Cordeau and Gavalas.\n",
    "    \"\"\"\n",
    "\n",
    "    if instance_type in ['Solomon']:\n",
    "        n_digits = 10.0\n",
    "\n",
    "    elif instance_type in ['Cordeau', 'Gavalas']:\n",
    "        n_digits = 100.0\n",
    "\n",
    "    n = instance_df.shape[0]\n",
    "    distm = np.zeros((n,n))\n",
    "    x = instance_df.x.values\n",
    "    y = instance_df.y.values\n",
    "\n",
    "    for i in range(0, n-1):\n",
    "        for j in range(i+1, n):\n",
    "            distm[i,j] = np.floor(n_digits*(np.sqrt((x[i]-x[j])**2+(y[i]-y[j])**2)))/n_digits\n",
    "            distm[j,i] = distm[i,j]\n",
    "\n",
    "    return distm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_data(instance, path, device):\n",
    "    instance_type = get_instance_type(instance)\n",
    "    df_inst = get_instance_df(instance, path, instance_type)\n",
    "    distm = get_distance_matrix(df_inst, instance_type)\n",
    "    raw_data = df_inst[['x', 'y', 'duration', 'ti', 'tf', 'prof', 'Total Time']].values\n",
    "    raw_data = torch.FloatTensor(raw_data).to(device)\n",
    "    raw_distm =  torch.FloatTensor(distm).to(device)\n",
    "\n",
    "    return raw_data, raw_distm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_df(instance_name, path, instance_type):\n",
    "\n",
    "    \"\"\"combine read instance, tests and parse to dataframe\"\"\"\n",
    "\n",
    "    OPENING_TIME_WINDOW_ABBREV_KEY = 'O'\n",
    "    CLOSING_TIME_WINDOW_ABBREV_KEY = 'C'\n",
    "    TOTAL_TIME_KEY = 'Total Time'\n",
    "\n",
    "\n",
    "    COLUMN_NAMES = ['vertex number', 'x coordinate', 'y coordinate',\n",
    "    'service duration or visiting time', 'profit of the location',\n",
    "    'not relevant 1', 'not relevant 2', 'not relevant 3',\n",
    "    'opening of time window', 'closing of time window']\n",
    "    COLUMN_NAMES = [s.replace(' ', '_') for s in COLUMN_NAMES]\n",
    "    COLUMN_NAMES_ABBREV = ['i', 'x', 'y', 'd', 'S', 'f', 'a', 'list', 'O', 'C']\n",
    "    VERTEX_NUMBER_COL = [i for i,n in enumerate(COLUMN_NAMES) if n=='vertex_number'][0]\n",
    "    COLS_OF_INT = ['i', 'x', 'y', 'd', OPENING_TIME_WINDOW_ABBREV_KEY,\n",
    "                   CLOSING_TIME_WINDOW_ABBREV_KEY, 'S', TOTAL_TIME_KEY]\n",
    "    COLS_OF_INT_NEW_NAMES = ['i', 'x', 'y', 'duration', 'ti', 'tf', 'prof', TOTAL_TIME_KEY]\n",
    "\n",
    "    standard2newnames_dict =  dict(((c, ca) for c, ca in zip(COLS_OF_INT, COLS_OF_INT_NEW_NAMES)))\n",
    "\n",
    "    instance_data = read_instance_data(instance_name, path)\n",
    "\n",
    "    # run tests\n",
    "    test_n_vert_1(instance_data, instance_type)\n",
    "    test_n_vert_2(instance_data, instance_type)\n",
    "    # test if it's a single day (we are not considering TOPTW instances)\n",
    "    test_n_vert_3(instance_data, instance_type)\n",
    "\n",
    "    if instance_type=='Gavalas':\n",
    "        df = parse_instance_data_Gavalas(instance_data)\n",
    "    else:\n",
    "        df = parse_instance_data(instance_data)\n",
    "\n",
    "    #change column names\n",
    "    COLS_OF_INT_NEW_NAMES = [standard2newnames_dict[s] for s in COLS_OF_INT]\n",
    "    df_ = df[COLS_OF_INT].copy()\n",
    "    df_.columns = COLS_OF_INT_NEW_NAMES\n",
    "    df_['inst_name'] = instance_name\n",
    "    df_['real_or_val'] = 'real'\n",
    "\n",
    "    df_ = df_.append(df_.loc[0])\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data(args, phase='train'):\n",
    "\n",
    "    df = get_instance_df(args.instance, cf.BENCHMARK_INSTANCES_PATH,\n",
    "                         args.instance_type)\n",
    "    dist_mat = get_distance_matrix(df, args.instance_type)\n",
    "    inp_real = df[['x', 'y', 'duration', 'ti', 'tf', 'prof', 'Total Time']].values\n",
    "\n",
    "    if phase=='train':\n",
    "        inp_real = [(torch.FloatTensor(inp_real).to(args.device),\n",
    "                torch.tensor(inp_real[0, pcf.OPENING_TIME_WINDOW_IDX]).to(args.device),\n",
    "                torch.FloatTensor(dist_mat).to(args.device))]\n",
    "\n",
    "        new_inp_real = [(args.instance, inp_real[0])]\n",
    "        return new_inp_real\n",
    "    else:\n",
    "        inp_real = [(torch.FloatTensor(inp_real).to(args.device),\n",
    "                 torch.FloatTensor(dist_mat).to(args.device))]\n",
    "        return inp_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_data(args, phase='train'):\n",
    "    path_string = os.path.abspath(os.getcwd())+'/{directory}/{file_name}'\n",
    "    inp_val_path = os.path.normpath(path_string.format(directory=args.val_dir, file_name=args.val_set_pt_file))\n",
    "    inp_val = torch.load(inp_val_path, map_location = args.map_location)\n",
    "\n",
    "    new_inp_val = [(args.instance, inst_data) for inst_data in inp_val]\n",
    "\n",
    "    if phase=='train':\n",
    "        return new_inp_val\n",
    "    else:\n",
    "\n",
    "        return inp_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFeatures():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(DynamicFeatures, self).__init__()\n",
    "\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.device = args.device\n",
    "\n",
    "    def make_dynamic_feat(self, data, current_time, current_poi_idx, dist_mat, batch_idx):\n",
    "\n",
    "        num_dyn_feat = 8\n",
    "        _ , sequence_size, input_size  = data.size()\n",
    "        batch_size = batch_idx.shape[0]\n",
    "\n",
    "        dyn_feat = torch.ones(batch_size, sequence_size, num_dyn_feat).to(self.device)\n",
    "\n",
    "        tour_start_time = data[0, 0, self.opening_time_window_idx]\n",
    "        max_tour_duration = data[0, 0, self.arrival_time_idx] - tour_start_time\n",
    "        arrive_j_times = current_time + dist_mat[current_poi_idx]\n",
    "\n",
    "        dyn_feat[:, :, 0] = (data[batch_idx, :, self.opening_time_window_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 1] = (data[batch_idx, :, self.closing_time_window_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 2] = (data[batch_idx, :, self.arrival_time_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 3] = (current_time - tour_start_time) / max_tour_duration\n",
    "\n",
    "\n",
    "        dyn_feat[:, :, 4] = (arrive_j_times - tour_start_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 5] = (data[batch_idx, :, self.opening_time_window_idx] - arrive_j_times) / max_tour_duration\n",
    "        dyn_feat[:, :, 6] = (data[batch_idx, :, self.closing_time_window_idx] - arrive_j_times) / max_tour_duration\n",
    "        dyn_feat[:, :, 7] = (data[batch_idx, :, self.arrival_time_idx] - arrive_j_times) / max_tour_duration\n",
    "\n",
    "        return dyn_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Norm Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_dependent_norm_const(instance_raw_data):\n",
    "    day_duration = int(instance_raw_data[:, pcf.CLOSING_TIME_WINDOW_IDX].max().item())\n",
    "    t_max_real = int(instance_raw_data[0, pcf.ARRIVAL_TIME_IDX].item()) # max instance arrival time\n",
    "    arrival_time_val_ub = t_max_real+int(pcf.SAMP_DAY_FRAC_INF*day_duration)\n",
    "    Tmax = int(max(day_duration, arrival_time_val_ub)) # max possible time value\n",
    "    Smax = int(torch.round(pcf.MULTIPLE_SCORE*instance_raw_data[1:-1, pcf.REWARD_IDX].max()).item()) # max score\n",
    "\n",
    "    return Tmax, Smax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_new_instance(inst_data, dist_mat, args):\n",
    "    instance_type = args.instance_type\n",
    "    sample_type  = args.sample_type\n",
    "\n",
    "    if instance_type=='Solomon':\n",
    "        n_digits = 10.0\n",
    "        xy_inf = 0.\n",
    "        xy_delta = 100.\n",
    "    elif instance_type=='Cordeau':\n",
    "        n_digits = 100.0\n",
    "        xy_inf = -100.\n",
    "        xy_delta = 200.\n",
    "    elif instance_type=='Gavalas':\n",
    "        n_digits = 100.0\n",
    "        xy_inf = 0.\n",
    "        xy_delta = 100.\n",
    "\n",
    "    poit = inst_data.clone()\n",
    "    n = inst_data.shape[0]\n",
    "\n",
    "    prof = inst_data[1:-1, pcf.REWARD_IDX]\n",
    "    durat_max = int(inst_data[1:-1, pcf.VIS_DURATION_TIME_IDX].max().item())\n",
    "\n",
    "    day_duration = int(inst_data[:, pcf.CLOSING_TIME_WINDOW_IDX].max().item())\n",
    "\n",
    "    t_init_real = int(inst_data[0, pcf.OPENING_TIME_WINDOW_IDX].item()) # starting time\n",
    "    t_max_real = int(inst_data[0, pcf.ARRIVAL_TIME_IDX].item()) # max arrival time\n",
    "\n",
    "    day_fract_inf = pcf.SAMP_DAY_FRAC_INF\n",
    "    t_min = int(pcf.SAMP_DAY_FRAC_INF*day_duration)\n",
    "    ub_t_init_val = pcf.UB_T_INIT_FRAC*day_duration\n",
    "    lb_t_max_val = pcf.LB_T_MAX_FRAC*day_duration\n",
    "\n",
    "    ub = int(torch.min(torch.tensor([ub_t_init_val,\n",
    "                                     t_max_real+int(day_fract_inf*day_duration)])))\n",
    "    t_init_val = torch.randint(int(t_init_real)-int(day_fract_inf*day_duration),\n",
    "                               ub,\n",
    "                               (1,))\n",
    "\n",
    "    lb = int(torch.max(torch.tensor([lb_t_max_val, int(t_init_val)+t_min])))\n",
    "    t_max_val = torch.randint(lb,\n",
    "                              t_max_real+int(day_fract_inf*day_duration),\n",
    "                              (1,))\n",
    "\n",
    "    Smax = int(torch.round(pcf.MULTIPLE_SCORE*prof.max()).item())\n",
    "    if sample_type == 'uni_samp':\n",
    "        new_scores = torch.randint(1, Smax, (n,))\n",
    "    elif sample_type == 'corr_samp':\n",
    "        new_scores_unbound = (Smax/durat_max)*inst_data[:, pcf.VIS_DURATION_TIME_IDX] + pcf.CORR_SCORE_STD*torch.randn(n, device=args.device)\n",
    "        new_scores = torch.round(torch.min(Smax*torch.ones(1, device=args.device),\n",
    "                                           torch.max(torch.ones(n, device=args.device),\n",
    "                                                     new_scores_unbound)))\n",
    "\n",
    "    poit[:, pcf.REWARD_IDX] = new_scores\n",
    "\n",
    "    #------------ correct first/last----------\n",
    "\n",
    "    poit[0, pcf.REWARD_IDX] = 0 # starting point\n",
    "    poit[n-1, pcf.REWARD_IDX] = 0 # ending point\n",
    "\n",
    "    poit[0, pcf.X_COORDINATE_IDX] = float(xy_inf+xy_delta*torch.rand(1)) # starting point\n",
    "    poit[n-1, pcf.X_COORDINATE_IDX] = poit[0, pcf.X_COORDINATE_IDX] # ending point\n",
    "\n",
    "    poit[0, pcf.Y_COORDINATE_IDX] = float(xy_inf+xy_delta*torch.rand(1)) # starting point\n",
    "    poit[n-1, pcf.Y_COORDINATE_IDX] = poit[0, pcf.Y_COORDINATE_IDX] # ending point\n",
    "\n",
    "    poit[:, pcf.ARRIVAL_TIME_IDX] = t_max_val*torch.ones(n)\n",
    "    poit[0, pcf.OPENING_TIME_WINDOW_IDX] = t_init_val*torch.ones(1)\n",
    "    poit[n-1, pcf.OPENING_TIME_WINDOW_IDX] = t_init_val*torch.ones(1)\n",
    "    poit[0, pcf.CLOSING_TIME_WINDOW_IDX] = t_max_val*torch.ones(1)\n",
    "    poit[n-1, pcf.CLOSING_TIME_WINDOW_IDX] = t_max_val*torch.ones(1)\n",
    "\n",
    "    start_time = t_init_val.clone().detach().to(args.device)\n",
    "    dist_matn = dist_mat.clone()\n",
    "\n",
    "    for j in range(1, n-1):\n",
    "        dist_matn[0, j] = float(torch.floor(n_digits*(torch.sqrt((poit[0,0]-poit[j,0])**2+(poit[0,1]-poit[j,1])**2))).item()/n_digits)\n",
    "        dist_matn[n-1, j] = dist_matn[0, j]\n",
    "\n",
    "        dist_matn[j, 0] = dist_matn[0, j]\n",
    "        dist_matn[j, n-1] = dist_matn[n-1, j]\n",
    "\n",
    "    return poit, start_time, dist_matn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(data, norm_dic):\n",
    "    datan = data.clone()\n",
    "    datan[:, pcf.X_COORDINATE_IDX] /= pcf.X_MAX\n",
    "    datan[:, pcf.Y_COORDINATE_IDX] /= pcf.X_MAX\n",
    "    datan[:, pcf.VIS_DURATION_TIME_IDX] /= (datan[:, pcf.VIS_DURATION_TIME_IDX].max())\n",
    "    datan[:, pcf.OPENING_TIME_WINDOW_IDX] /= norm_dic['Tmax']\n",
    "    datan[:, pcf.CLOSING_TIME_WINDOW_IDX ] /= norm_dic['Tmax']\n",
    "    datan[:, pcf.REWARD_IDX] /= norm_dic['Smax']\n",
    "    datan[:, pcf.ARRIVAL_TIME_IDX] /= norm_dic['Tmax']\n",
    "\n",
    "    return datan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(raw_data, raw_dist_mat, norm_dic, run_episode, opt, args):\n",
    "\n",
    "    new_data, start_time, dist_mat = sample_new_instance(raw_data, raw_dist_mat, args)\n",
    "    new_data_scaled = data_scaler(new_data, norm_dic[args.instance])\n",
    "    bnew_data, bnew_data_scaled = samples2batch(new_data, new_data_scaled, args.batch_size)\n",
    "\n",
    "    run_episode.train()\n",
    "    opt.zero_grad()\n",
    "    actions, log_prob, entropy, step_mask = run_episode(bnew_data, bnew_data_scaled, start_time, dist_mat, 'stochastic')\n",
    "\n",
    "    rewards = reward_fn(new_data, actions, args.device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    av_rew = rewards.mean()\n",
    "    min_rew = rewards.min()\n",
    "    max_rew = rewards.max()\n",
    "\n",
    "    advantage = (rewards - av_rew) #advantage\n",
    "\n",
    "    res = advantage.unsqueeze(1)*log_prob + args.beta*entropy\n",
    "\n",
    "    loss = -res[step_mask].sum()/args.batch_size\n",
    "\n",
    "    loss.backward(retain_graph=False)\n",
    "    torch.nn.utils.clip_grad_norm_(run_episode.neuralnet.parameters(), args.max_grad_norm)\n",
    "    opt.step()\n",
    "\n",
    "    return av_rew.item(), min_rew.item(), max_rew.item(), loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples2batch(new_data, new_data_scaled, batch_size):\n",
    "    bnew_data = new_data.expand(batch_size, -1, -1)\n",
    "    bnew_data_scaled = new_data_scaled.expand(batch_size, -1, -1)\n",
    "    return bnew_data, bnew_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(data, sample_solution, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tensor of shape [batch_size] containing rewards\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = sample_solution[0].shape[0]\n",
    "    tour_reward = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    for act_id in sample_solution:\n",
    "        tour_reward += data[act_id, pcf.REWARD_IDX].squeeze(0)\n",
    "\n",
    "    return tour_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_step=5000):\n",
    "    \"\"\"Decay learning rate by a factor of 0.96 every lr_decay_epoch epochs.\n",
    "       Lower_bounded at 0.00001\"\"\"\n",
    "    lr = init_lr * (0.96**(epoch // lr_decay_step))\n",
    "    if lr < 0.00001:\n",
    "        lr = 0.00001\n",
    "\n",
    "    if epoch % lr_decay_step == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(inp_val, run_episode, inst_norm_dic, device):\n",
    "    reward_val =  torch.tensor(0.0).to(device)\n",
    "    rew_dict = {}\n",
    "    for k, (inst_name, data) in enumerate(inp_val):\n",
    "        inst_data, start_time, dist_mat = data\n",
    "        rew = test_model(inst_data, start_time, dist_mat, inst_name, inst_norm_dic, run_episode, device)\n",
    "        reward_val += rew\n",
    "        key_str = inst_name + '_' + str(k)\n",
    "        rew_dict[key_str] = rew\n",
    "\n",
    "    return rew_dict, reward_val.item()/len(inp_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data, start_time, dist_mat, inst, inst_norm_dic, run_episode, device):\n",
    "    with torch.no_grad():\n",
    "        data_scaled = data_scaler(data, inst_norm_dic[inst])\n",
    "        bdata, bdata_scaled = data.unsqueeze(0), data_scaled.unsqueeze(0)\n",
    "        actions, log_prob, entropy, step_mask = run_episode(bdata, bdata_scaled, start_time, dist_mat, 'greedy')\n",
    "        reward = reward_fn(data, actions, device)\n",
    "\n",
    "        return reward.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelUtils():\n",
    "    def __init__(self, args):\n",
    "        super(ModelUtils, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.vis_duration_time_idx = pcf.VIS_DURATION_TIME_IDX\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "\n",
    "    def feasibility_control(self, braw_inputs, mask, dist_mat, pres_act, present_time, batch_idx, first_step=False):\n",
    "\n",
    "        done = False\n",
    "        maskk = mask.clone()\n",
    "        step_batch_size = batch_idx.shape[0]\n",
    "\n",
    "        arrivej = dist_mat[pres_act] + present_time\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), braw_inputs[:, :, self.opening_time_window_idx]-arrivej)\n",
    "\n",
    "        c1 = arrivej + waitj <= braw_inputs[:, :, self.closing_time_window_idx]\n",
    "        c2 = arrivej + waitj + braw_inputs[:, :, self.vis_duration_time_idx] + dist_mat[:, -1] <= braw_inputs[0, 0, self.arrival_time_idx]\n",
    "\n",
    "        if not first_step:\n",
    "            maskk[batch_idx, pres_act] = 0\n",
    "\n",
    "        maskk[batch_idx] = maskk[batch_idx] * c1 * c2\n",
    "\n",
    "        if maskk[:, -1].any() == 0:\n",
    "            done = True\n",
    "        return done, maskk\n",
    "\n",
    "\n",
    "    def one_step_update(self, raw_inputs_b, dist_mat, pres_action, future_action, present_time, batch_idx, batch_size):\n",
    "\n",
    "        present_time_b = torch.zeros(batch_size, 1, device=self.device)\n",
    "        pres_actions_b = torch.zeros(batch_size, dtype=torch.int64, device=self.device)\n",
    "        step_mask_b = torch.zeros(batch_size, 1, device=self.device, requires_grad=False, dtype=torch.bool)\n",
    "\n",
    "        arrive_j = dist_mat[pres_action, future_action].unsqueeze(1) + present_time\n",
    "        wait_j = torch.max(torch.FloatTensor([0.0]).to(self.device),\n",
    "                           raw_inputs_b[batch_idx, future_action, self.opening_time_window_idx].unsqueeze(1)-arrive_j)\n",
    "        present_time = arrive_j + wait_j + raw_inputs_b[batch_idx, future_action, self.vis_duration_time_idx].unsqueeze(1)\n",
    "\n",
    "        present_time_b[batch_idx] = present_time\n",
    "\n",
    "        pres_actions_b[batch_idx] = future_action\n",
    "        step_mask_b[batch_idx] = 1\n",
    "\n",
    "        return pres_actions_b, present_time_b, step_mask_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead():\n",
    "    def __init__(self, args):\n",
    "        super(Lookahead, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.vis_duration_time_idx = pcf.VIS_DURATION_TIME_IDX\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "\n",
    "    def adjacency_matrix(self, braw_inputs, mask, dist_mat, pres_act, present_time):\n",
    "        # feasible neighborhood for each node\n",
    "        maskk = mask.clone()\n",
    "        step_batch_size, npoints = mask.shape\n",
    "\n",
    "        #one step forward update\n",
    "        arrivej = dist_mat[pres_act] + present_time\n",
    "        farrivej = arrivej.view(step_batch_size, npoints)\n",
    "        tw_start = braw_inputs[:, :, self.opening_time_window_idx]\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), tw_start-farrivej)\n",
    "        durat = braw_inputs[:, : , self.vis_duration_time_idx]\n",
    "\n",
    "        fpresent_time = farrivej + waitj + durat\n",
    "        fpres_act = torch.arange(0, npoints, device=self.device).expand(step_batch_size, -1)\n",
    "\n",
    "        # feasible neighborhood for each node\n",
    "        adj_mask = maskk.unsqueeze(1).repeat(1, npoints, 1)\n",
    "        arrivej = dist_mat.expand(step_batch_size, -1, -1) + fpresent_time.unsqueeze(2)\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), tw_start.unsqueeze(2)-arrivej)\n",
    "\n",
    "        tw_end = braw_inputs[:, :, self.closing_time_window_idx]\n",
    "        ttime = braw_inputs[:, 0, self.arrival_time_idx]\n",
    "\n",
    "        dlast = dist_mat[:, -1].unsqueeze(0).expand(step_batch_size, -1)\n",
    "\n",
    "        c1 = arrivej + waitj <= tw_end.unsqueeze(1)\n",
    "        c2 = arrivej + waitj + durat.unsqueeze(1) + dlast.unsqueeze(1) <= ttime.unsqueeze(1).unsqueeze(1).expand(-1, npoints, npoints)\n",
    "        adj_mask = adj_mask * c1 * c2\n",
    "\n",
    "        # self-loop\n",
    "        idx = torch.arange(0, npoints, device=self.device).expand(step_batch_size, -1)\n",
    "        adj_mask[:, idx, idx] = 1\n",
    "\n",
    "        return adj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunEpisode(nn.Module):\n",
    "\n",
    "    def __init__(self, neuralnet, args):\n",
    "        super(RunEpisode, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.neuralnet = neuralnet\n",
    "        self.dyn_feat = DynamicFeatures(args)\n",
    "        self.lookahead = Lookahead(args)\n",
    "        self.mu = ModelUtils(args)\n",
    "\n",
    "    def forward(self, binputs, bdata_scaled, start_time, dist_mat, infer_type):\n",
    "\n",
    "        self.batch_size, sequence_size, input_size = binputs.size()\n",
    "\n",
    "        h_0, c_0 = self.neuralnet.decoder.hidden_0\n",
    "\n",
    "        dec_hidden = (h_0.expand(self.batch_size, -1), c_0.expand(self.batch_size, -1))\n",
    "\n",
    "        mask = torch.ones(self.batch_size, sequence_size, device=self.device, requires_grad=False, dtype = torch.uint8)\n",
    "\n",
    "        bpresent_time = start_time*torch.ones(self.batch_size, 1, device=self.device)\n",
    "\n",
    "        llog_probs, lactions, lstep_mask, lentropy = [], [], [], []\n",
    "\n",
    "        bpres_actions = torch.zeros(self.batch_size, dtype=torch.int64, device=self.device)\n",
    "\n",
    "        batch_idx = torch.arange(0, self.batch_size, device=self.device)\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat, bpres_actions,\n",
    "                                                 bpresent_time, batch_idx, first_step=True)\n",
    "\n",
    "        adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        # encoder first forward pass\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time, bpres_actions, dist_mat, batch_idx)\n",
    "        emb1 = self.neuralnet.sta_emb(bdata_scaled)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "        _, _, enc_outputs = self.neuralnet(enc_inputs, enc_inputs, adj_mask, enc_inputs, dec_hidden, mask, first_step=True)\n",
    "\n",
    "        decoder_input = enc_outputs[batch_idx, bpres_actions]\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time, batch_idx)\n",
    "        adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        # encoder/decoder forward pass\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time,\n",
    "                                                      bpres_actions, dist_mat, batch_idx)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "        policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs, adj_mask, decoder_input, dec_hidden, mask)\n",
    "\n",
    "        lactions.append(bpres_actions)\n",
    "\n",
    "        # Starting the trip\n",
    "        while not done:\n",
    "\n",
    "            future_actions, log_probs, entropy = self.select_actions(policy, infer_type)\n",
    "\n",
    "            bpres_actions, bpresent_time, bstep_mask = self.mu.one_step_update(binputs, dist_mat, bpres_actions[batch_idx],\n",
    "                                                                               future_actions, bpresent_time[batch_idx],\n",
    "                                                                               batch_idx, self.batch_size)\n",
    "\n",
    "            blog_probs = torch.zeros(self.batch_size, 1, dtype=torch.float32).to(self.device)\n",
    "            blog_probs[batch_idx] = log_probs.unsqueeze(1)\n",
    "\n",
    "            bentropy = torch.zeros(self.batch_size,1,dtype=torch.float32).to(self.device)\n",
    "            bentropy[batch_idx] = entropy.unsqueeze(1)\n",
    "\n",
    "            llog_probs.append(blog_probs)\n",
    "            lactions.append(bpres_actions)\n",
    "            lstep_mask.append(bstep_mask)\n",
    "            lentropy.append(bentropy)\n",
    "\n",
    "            done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat,\n",
    "                                                     bpres_actions[batch_idx], bpresent_time[batch_idx],\n",
    "                                                     batch_idx)\n",
    "\n",
    "            if done: break\n",
    "            sub_batch_idx = torch.nonzero(mask[batch_idx][:,-1], as_tuple=False).squeeze(1)\n",
    "\n",
    "            batch_idx = torch.nonzero(mask[:,-1], as_tuple=False).squeeze(1)\n",
    "\n",
    "            adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask[batch_idx], dist_mat, bpres_actions[batch_idx], bpresent_time[batch_idx])\n",
    "\n",
    "            #update decoder input and hidden\n",
    "            decoder_input = enc_outputs[sub_batch_idx, bpres_actions[sub_batch_idx]]\n",
    "            dec_hidden = (dec_hidden[0][sub_batch_idx], dec_hidden[1][sub_batch_idx])\n",
    "\n",
    "            # encoder/decoder forward pass\n",
    "            bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time[batch_idx], bpres_actions[batch_idx], dist_mat, batch_idx)\n",
    "            emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "            enc_inputs = torch.cat((emb1[batch_idx],emb2), dim=2)\n",
    "\n",
    "            policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs[sub_batch_idx], adj_mask, decoder_input, dec_hidden, mask[batch_idx])\n",
    "\n",
    "        return lactions, torch.cat(llog_probs, dim=1), torch.cat(lentropy, dim=1), torch.cat(lstep_mask, dim=1)\n",
    "\n",
    "\n",
    "    def select_actions(self, policy, infer_type):\n",
    "\n",
    "        if infer_type == 'stochastic':\n",
    "            m = Categorical(policy)\n",
    "            act_ind = m.sample()\n",
    "            log_select =  m.log_prob(act_ind)\n",
    "            poli_entro = m.entropy()\n",
    "        elif infer_type == 'greedy':\n",
    "            prob, act_ind = torch.max(policy, 1)\n",
    "            log_select =  prob.log()\n",
    "            poli_entro =  torch.zeros(self.batch_size, requires_grad=False).to(self.device)\n",
    "\n",
    "        return act_ind, log_select, poli_entro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(inp_val, inp_real, raw_data, run_episode, args):\n",
    "\n",
    "    raw_data, raw_dist_mat = inp_real[0][1][0], inp_real[0][1][2]\n",
    "    reward_total, min_reward_total, max_reward_total, loss_total = 0, 0, 0, 0\n",
    "    training_history = []\n",
    "    model_opt = optim.Adam(run_episode.neuralnet.parameters(), lr=args.lr)\n",
    "    step_dict = {}\n",
    "\n",
    "    for epoch in tqdm(range(1,args.nepocs+1)):\n",
    "\n",
    "        avg_reward, min_reward, max_reward, loss = train_model(raw_data,\n",
    "                                                                  raw_dist_mat,\n",
    "                                                                  norm_dic,\n",
    "                                                                  run_episode,\n",
    "                                                                  model_opt,\n",
    "                                                                  args)\n",
    "\n",
    "        reward_total += avg_reward\n",
    "        min_reward_total += min_reward\n",
    "        max_reward_total += max_reward\n",
    "        loss_total += loss\n",
    "\n",
    "        exp_lr_scheduler(model_opt, epoch, init_lr=args.lr)\n",
    "\n",
    "\n",
    "        if epoch==0 or epoch % args.nprint == 0:\n",
    "            print(\"Epoch %s\" % str(epoch))\n",
    "            rew_dict, avg_reward_val = validation(inp_val, run_episode, norm_dic, args.device)\n",
    "            _, avg_reward_real  = validation(inp_real, run_episode, norm_dic, args.device)\n",
    "            step_dict[epoch] = rew_dict\n",
    "\n",
    "            if epoch == 0:\n",
    "                avg_loss = loss_total\n",
    "                avg_reward_total = reward_total\n",
    "                avg_min_reward_total = min_reward_total\n",
    "                avg_max_reward_total = max_reward_total\n",
    "\n",
    "                training_history.append([epoch, reward_total, min_reward_total, max_reward_total,\n",
    "                                         avg_reward_val, avg_reward_real, loss_total])\n",
    "\n",
    "            else:\n",
    "                avg_loss = loss_total / args.nprint\n",
    "                avg_reward_total = reward_total / args.nprint\n",
    "                avg_min_reward_total = min_reward_total / args.nprint\n",
    "                avg_max_reward_total = max_reward_total / args.nprint\n",
    "\n",
    "\n",
    "                training_history.append([epoch, avg_reward_total, avg_min_reward_total, avg_max_reward_total,\n",
    "                                         avg_reward_val, avg_reward_real, avg_loss])\n",
    "\n",
    "\n",
    "            print(N_DASHES*'-')\n",
    "            print(\"Average total loss: %s\" % avg_loss)\n",
    "            print(\"Average train mean reward: %s\" % avg_reward_total)\n",
    "            print(\"Average train max reward: %s\" % avg_max_reward_total)\n",
    "            print(\"Average train min reward: %s\" % avg_min_reward_total)\n",
    "            print(\"Validation reward: %2.3f\"  % (avg_reward_val))\n",
    "            print(\"Real instance reward: %2.3f\"  % (avg_reward_real))\n",
    "\n",
    "            reward_total, min_reward_total, max_reward_total, loss_total = 0, 0, 0, 0\n",
    "\n",
    "        if epoch % args.nsave == 0 and not args.debug:\n",
    "            print('saving model')\n",
    "            torch.save(run_episode.neuralnet.state_dict(), args.save_w_dir+'/model_'+str(epoch)+'.pkl')\n",
    "\n",
    "    return training_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DotMap(dict(\n",
    "    batch_size=32, \n",
    "    beta=0.01, \n",
    "    debug=False, \n",
    "    device='cpu',\n",
    "    device_name='cpu',\n",
    "    ff_dim=256, \n",
    "    instance='t101', \n",
    "    instance_type='Gavalas', \n",
    "    lr=0.0001, \n",
    "    map_location={'cpu': 'cpu'}, \n",
    "    max_grad_norm=1,\n",
    "    model_name='testing_1', \n",
    "    n_heads=8, \n",
    "    n_layers=2, \n",
    "    ndfeatures=8, \n",
    "    nepocs=1, \n",
    "    nfeatures=7, \n",
    "    nprint=1, \n",
    "    nsave=1, \n",
    "    rnn_hidden=128, \n",
    "    sample_type='corr_samp', \n",
    "    save_h_dir='./results/t101/outputs/model_testing_1_corr_samp', \n",
    "    save_h_file='training_history.csv', \n",
    "    save_w_dir='./results/t101/model_w/model_testing_1_corr_samp', \n",
    "    seed=2925, \n",
    "    use_checkpoint=False, \n",
    "    val_dir='./data/generated//t101', \n",
    "    val_set_pt_file='inp_val_corr_samp.pt'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-836cb4b14d20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for reproducibility\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mN_DASHES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cuda:1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# for reproducibility\"\n",
    "N_DASHES = 40\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if str(args.device) in ['cuda', 'cuda:0', 'cuda:1']:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# create directories for saving\n",
    "if not args.debug:\n",
    "    os.makedirs(args.save_h_dir) if not os.path.exists(args.save_h_dir) else None\n",
    "    os.makedirs(args.save_w_dir) if not os.path.exists(args.save_w_dir) else None\n",
    "\n",
    "# get val and real instance scores\n",
    "inp_real = get_real_data(args, phase='train')\n",
    "inp_val = get_val_data(args, phase='train')\n",
    "\n",
    "# get Tmax and Smax\n",
    "raw_data = inp_real[0][1][0]\n",
    "Tmax, Smax = instance_dependent_norm_const(raw_data)\n",
    "norm_dic = {args.instance: {'Tmax': Tmax, 'Smax': Smax}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_history(training_history, file_path, args):\n",
    "    TRAIN_HIST_COLUMNS = ['epoch', 'avg_reward_train',\n",
    "        'min_reward_train', 'max_reward_train', 'avg_reward_val_'+args.sample_type,\n",
    "        'avg_reward_real', 'tloss_train']\n",
    "\n",
    "    df = pd.DataFrame(training_history, columns = TRAIN_HIST_COLUMNS)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:26<00:00, 26.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Average total loss: 594.6771850585938\n",
      "Average train mean reward: 185.1875\n",
      "Average train max reward: 291.0\n",
      "Average train min reward: 1.0\n",
      "Validation reward: 207.500\n",
      "Real instance reward: 237.000\n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model = RecPointerNetwork(args.nfeatures, args.ndfeatures, args.rnn_hidden, args).to(args.device)\n",
    "run_episode = RunEpisode(model, args)\n",
    "\n",
    "training_history = train_loop(inp_val, inp_real, norm_dic, run_episode, args)\n",
    "\n",
    "# save\n",
    "if not args.debug:\n",
    "    file_path = '%s/%s' % (args.save_h_dir, args.save_h_file)\n",
    "    save_training_history(training_history, file_path, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optw_env]",
   "language": "python",
   "name": "conda-env-optw_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
