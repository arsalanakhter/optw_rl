{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "When we take a dot product of query and key values, the variance of the result of the dot product may be scaled by $d$, which is the dimension of the key/query vectors. To ensure that the variance of the dot product still remains the same regardless of vector length, the scaled dot-product attention scoring function is used.\n",
    "\n",
    "Assuming $\\mathbf{Q} \\in \\mathbb{R}^{n \\times d}$, keys $\\mathbf{K}\\in \\mathbb{R}^{m \\times d}$, values $\\mathbf{V}\\in \\mathbb{R}^{m \\times v}$ we do\n",
    "\n",
    "$$\\text{softmax} \\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right)\\mathbf{V} \\quad \\in \\mathbb{R}^{n\\times v}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = np.sqrt(d_k)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        # q: [b_size x len_q x d_k]\n",
    "        # k: [b_size x len_k x d_k]\n",
    "        # v: [b_size x len_v x d_v] note: (len_k == len_v)\n",
    "        # Batch-Matrix Multiplication\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) / self.scale_factor  # attn: [b_size x len_q x len_k]\n",
    "        if attn_mask is not None:\n",
    "        #    assert attn_mask.size() == attn.size()\n",
    "            attn.data.masked_fill_(attn_mask==0, -1e32)\n",
    "\n",
    "        attn = self.softmax(attn )\n",
    "        outputs = torch.bmm(attn, v) # outputs: [b_size x len_q x d_v]\n",
    "        return outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From : http://nlp.seas.harvard.edu/2018/04/01/attention.html#position-wise-feed-forward-networks\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{model}$, and the inner-layer has dimensionality $d_{ff}$.\n",
    "\n",
    "Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x # inputs: [b_size x len_q x d_model]\n",
    "        outputs = self.w_2(F.relu(self.w_1(x)))\n",
    "        return self.layer_norm(residual + outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(_MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.w_q = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_k))\n",
    "        self.w_k = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_k))\n",
    "        self.w_v = nn.Parameter(torch.FloatTensor(n_heads, d_model, self.d_v))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None, is_adj=True):\n",
    "        (d_k, d_v, d_model, n_heads) = (self.d_k, self.d_v, self.d_model, self.n_heads)\n",
    "        b_size = k.size(0)\n",
    "\n",
    "        q_s = q.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_q x d_model]\n",
    "        k_s = k.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_k x d_model]\n",
    "        v_s = v.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_v x d_model]\n",
    "\n",
    "        q_s = torch.bmm(q_s, self.w_q).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_q x d_k]\n",
    "        k_s = torch.bmm(k_s, self.w_k).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_k x d_k]\n",
    "        v_s = torch.bmm(v_s, self.w_v).view(b_size * n_heads, -1, d_v)  # [b_size * n_heads x len_v x d_v]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if is_adj:\n",
    "                outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_heads, 1, 1))\n",
    "            else:\n",
    "                outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.unsqueeze(1).repeat(n_heads, 1, 1))\n",
    "        else:\n",
    "            outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=None)\n",
    "\n",
    "        return torch.split(outputs, b_size, dim=0), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.attention = _MultiHeadAttention(d_model, n_heads)\n",
    "        self.proj = nn.Linear(n_heads * self.d_k, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask = None, is_adj = True):\n",
    "        # q: [b_size x len_q x d_model]\n",
    "        # k: [b_size x len_k x d_model]\n",
    "        # v: [b_size x len_v x d_model] note (len_k == len_v)\n",
    "        residual = q\n",
    "        # outputs: a list of tensors of shape [b_size x len_q x d_v] (length: n_heads)\n",
    "        outputs, attn = self.attention(q, k, v, attn_mask=attn_mask, is_adj=is_adj)\n",
    "        # concatenate 'n_heads' multi-head attentions\n",
    "        outputs = torch.cat(outputs, dim=-1)\n",
    "        # project back to residual size, result_size = [b_size x len_q x d_model]\n",
    "        outputs = self.proj(outputs)\n",
    "\n",
    "        return self.layer_norm(residual + outputs), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single Encoder Layer consists of One MultiHead Attention Layer and one PositionWiseFeedForward layer. Its forward pass consists of Encoder input, Recursive Encoder input and self attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inp, rec_enc_inp, self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inp, rec_enc_inp, enc_inp, attn_mask=self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the encoder here consists of:\n",
    "1. A layer for static features to number of hidden layers / 2\n",
    "2. A layer for dynamic features to number of hidden layers / 2.\n",
    "\n",
    "These layers constitute the input layers. Here's where we give input to the network.\n",
    "\n",
    "Then, we have the hidden layers. (May be Embeddings? I don't know.). But they are a Module List of Layers each of type EncoderLayer, having as input the hidden_size and output as $d_{ff}$, which are the dimensions of the feed forward layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features_dim, dfeatures_dim, hidden_size, args):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        n_heads = args.n_heads # number of heads\n",
    "        d_ff = args.ff_dim # feed_forward_hidden\n",
    "        n_layers = args.n_layers # number of Layers\n",
    "\n",
    "        self.L1 = nn.Linear(features_dim, hidden_size//2) # for static features\n",
    "        self.L2 = nn.Linear(dfeatures_dim, hidden_size//2) # for dynamic features\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_size, d_ff, n_heads) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, emb_inp, rec_inp, mask, dummy_arg):\n",
    "        for layer in self.layers:\n",
    "            emb_inp, _ = layer(emb_inp, rec_inp, mask)\n",
    "\n",
    "        return emb_inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Parameter(torch.zeros((hidden_size, 1), requires_grad=True))\n",
    "\n",
    "        self.first_h_0 = nn.Parameter(torch.FloatTensor(1, hidden_size), requires_grad=True)\n",
    "        self.first_h_0.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))\n",
    "\n",
    "        self.c0 = nn.Parameter(torch.FloatTensor( 1, hidden_size),requires_grad=True)\n",
    "        self.c0.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))\n",
    "\n",
    "        self.hidden_0 = (self.first_h_0, self.c0)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, enc_outputs, mask):\n",
    "        hidden = self.lstm(input, hidden)\n",
    "        w1e = self.W1(enc_outputs)\n",
    "        w2h = self.W2(hidden[0]).unsqueeze(1)\n",
    "        u = torch.tanh(w1e + w2h)\n",
    "        a = u.matmul(self.V)\n",
    "        a = 10*torch.tanh(a).squeeze(2)\n",
    "\n",
    "        policy = F.softmax(a + mask.float().log(), dim=1)\n",
    "\n",
    "        return policy, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Pointer Network here takes as input 1) static features, 2) Dynamic Features 3) Number of dimensions of the hidden layers. It contains a decoder block and an encoder block.\n",
    "\n",
    "The parameters are initialized with Xavier initialization.\n",
    "\n",
    "The goal of Xavier Initialization is to initialize the weights such that the variance of the activations are the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing. (Taken from https://cs230.stanford.edu/section/4/)\n",
    "\n",
    "(Xavier initialization is designed to work well with tanh or sigmoid activation functions. For ReLU activations, look into He initialization, which follows a very similar derivation. https://cs230.stanford.edu/section/4/ My note: Need to check this further)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecPointerNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, features_dim, dfeatures_dim, hidden_dim, args):\n",
    "        super(RecPointerNetwork, self).__init__()\n",
    "\n",
    "        self.features_dim = features_dim\n",
    "        self.dfeatures_dim = dfeatures_dim\n",
    "        self.use_checkpoint = args.use_checkpoint\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder = Decoder(hidden_dim)\n",
    "        self.encoder = Encoder(features_dim, dfeatures_dim, hidden_dim, args)\n",
    "        # see https://discuss.pytorch.org/t/checkpoint-with-no-grad-requiring-inputs-problem/19117/11\n",
    "        self.dummy_tensor = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def _load_model_weights(self, path_string, device):\n",
    "        self.load_state_dict(torch.load(path_string, map_location=device))\n",
    "\n",
    "\n",
    "    def forward(self, enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step=False):\n",
    "        policy, dec_hidden, enc_outputs = self._one_step(enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step)\n",
    "        return policy, dec_hidden, enc_outputs\n",
    "\n",
    "    def _one_step(self, enc_inputs, enc_hidden, adj_mask, dec_input, dec_hidden, mask, first_step):\n",
    "        if self.use_checkpoint:\n",
    "            enc_outputs = checkpoint(self.encoder, enc_inputs, enc_hidden, adj_mask, self.dummy_tensor)\n",
    "        else:\n",
    "            enc_outputs = self.encoder(enc_inputs, enc_hidden, adj_mask, self.dummy_tensor)\n",
    "\n",
    "        if first_step:\n",
    "            return  None, None, enc_outputs\n",
    "        else:\n",
    "            policy, dec_hidden = self.decoder(dec_input, dec_hidden, enc_outputs, mask)\n",
    "            return policy, dec_hidden, enc_outputs\n",
    "\n",
    "    def sta_emb(self, sta_inp):\n",
    "        return torch.tanh(self.encoder.L1(sta_inp))\n",
    "\n",
    "    def dyn_emb(self, dyn_inp):\n",
    "        return torch.tanh(self.encoder.L2(dyn_inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    def __init__(self, neuralnet, args):\n",
    "        super(BeamSearch, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.neuralnet = neuralnet\n",
    "        self.dyn_feat = DynamicFeatures(args)\n",
    "        self.lookahead = Lookahead(args)\n",
    "        self.mu = ModelUtils(args)\n",
    "\n",
    "    def forward(self, inputs, data_scaled, start_time, dist_mat, infer_type, beam_size):\n",
    "        self.beam_size = beam_size\n",
    "        _, sequence_size, input_size = inputs.size()\n",
    "\n",
    "\n",
    "        # first step  - node 0\n",
    "        bpresent_time = start_time*torch.ones(1, 1, device=self.device)\n",
    "\n",
    "        mask = torch.ones(1, sequence_size, device=self.device, requires_grad=False, dtype= torch.uint8)\n",
    "        bpres_actions = torch.zeros(1, dtype=torch.int64,device=self.device)\n",
    "        beam_idx = torch.arange(0, 1, device=self.device)\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                 mask, dist_mat, bpres_actions, bpresent_time,\n",
    "                                                 torch.arange(0, mask.shape[0], device=self.device),\n",
    "                                                 first_step=True)\n",
    "        adj_mask = self.lookahead.adjacency_matrix(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                   mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        h_0, c_0 = self.neuralnet.decoder.hidden_0\n",
    "        dec_hidden = (h_0.expand(1, -1), c_0.expand(1, -1))\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        # encoder first forward pass\n",
    "        bdata_scaled = data_scaled.expand(1,-1,-1)\n",
    "        sum_log_probs = torch.zeros(1, device=self.device).float()\n",
    "\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(inputs.expand(1,-1,-1), bpresent_time, bpres_actions, dist_mat, beam_idx)\n",
    "        emb1 = self.neuralnet.sta_emb(bdata_scaled)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1, emb2), dim=2)\n",
    "\n",
    "        _, _, enc_outputs = self.neuralnet(enc_inputs, enc_inputs, adj_mask, enc_inputs, dec_hidden, mask, first_step=True)\n",
    "\n",
    "        decoder_input = enc_outputs[beam_idx, bpres_actions]\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                 mask, dist_mat, bpres_actions, bpresent_time,\n",
    "                                                 torch.arange(0, mask.shape[0], device=self.device))\n",
    "        adj_mask = self.lookahead.adjacency_matrix(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                   mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        # encoder/decoder forward pass\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(inputs.expand(beam_idx.shape[0], -1, -1), bpresent_time, bpres_actions, dist_mat, beam_idx)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "        policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs, adj_mask, decoder_input, dec_hidden, mask)\n",
    "\n",
    "        future_actions, log_probs, beam_idx = self.select_actions(policy, sum_log_probs, mask, infer_type)\n",
    "        # info update\n",
    "        h_step = torch.index_select(dec_hidden[0], dim=0, index = beam_idx)\n",
    "        c_step = torch.index_select(dec_hidden[1], dim=0, index = beam_idx)\n",
    "        dec_hidden = (h_step,c_step)\n",
    "\n",
    "        mask = torch.index_select(mask, dim=0, index=beam_idx)\n",
    "        bpresent_time = torch.index_select(bpresent_time, dim=0, index=beam_idx)\n",
    "        bpres_actions = torch.index_select(bpres_actions, dim=0, index=beam_idx)\n",
    "        enc_outputs  = torch.index_select(enc_outputs, dim=0, index=beam_idx)\n",
    "        sum_log_probs = torch.index_select(sum_log_probs, dim=0, index=beam_idx)\n",
    "\n",
    "        emb1 = torch.index_select(emb1, dim=0, index=beam_idx)\n",
    "\n",
    "        # initialize buffers\n",
    "        bllog_probs = torch.zeros(bpres_actions.shape[0], sequence_size, device=self.device).float()\n",
    "        blactions = torch.zeros(bpres_actions.shape[0], sequence_size, device=self.device).long()\n",
    "\n",
    "        sum_log_probs += log_probs.squeeze(0).detach()\n",
    "\n",
    "        blactions[:, step] = bpres_actions\n",
    "\n",
    "        final_log_probs, final_actions, lstep_mask = [], [], []\n",
    "\n",
    "        # Starting the trip\n",
    "        while not done:\n",
    "\n",
    "            future_actions = future_actions.squeeze(0)\n",
    "\n",
    "            beam_size = bpres_actions.shape[0]\n",
    "            bpres_actions, bpresent_time, bstep_mask = \\\n",
    "                self.mu.one_step_update(inputs.expand(beam_size, -1, -1), dist_mat,\n",
    "                                        bpres_actions, future_actions, bpresent_time,\n",
    "                                        torch.arange(0,beam_size,device=self.device),\n",
    "                                        beam_size)\n",
    "\n",
    "            bllog_probs[:, step] = log_probs\n",
    "            blactions[:, step+1] = bpres_actions\n",
    "            step+=1\n",
    "\n",
    "            done, mask = self.mu.feasibility_control(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                     mask, dist_mat, bpres_actions, bpresent_time,\n",
    "                                                     torch.arange(0, mask.shape[0], device=self.device))\n",
    "            adj_mask = self.lookahead.adjacency_matrix(inputs.expand(beam_idx.shape[0], -1, -1),\n",
    "                                                       mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "            active_beam_idx = torch.nonzero(mask[:, -1], as_tuple=False).squeeze(1)\n",
    "            end_beam_idx = torch.nonzero((mask[:, -1]==0), as_tuple=False).squeeze(1)\n",
    "\n",
    "            if end_beam_idx.shape[0]>0:\n",
    "\n",
    "                final_log_probs.append(torch.index_select(bllog_probs, dim=0, index=end_beam_idx))\n",
    "                final_actions.append(torch.index_select(blactions, dim=0, index=end_beam_idx))\n",
    "\n",
    "                # ending seq info update\n",
    "                h_step = torch.index_select(dec_hidden[0], dim=0, index = active_beam_idx)\n",
    "                c_step = torch.index_select(dec_hidden[1], dim=0, index = active_beam_idx)\n",
    "                dec_hidden = (h_step,c_step)\n",
    "\n",
    "                mask = torch.index_select(mask, dim=0, index=active_beam_idx)\n",
    "                adj_mask = torch.index_select(adj_mask, dim=0, index=active_beam_idx)\n",
    "\n",
    "                bpresent_time = torch.index_select(bpresent_time, dim=0, index=active_beam_idx)\n",
    "                bpres_actions = torch.index_select(bpres_actions, dim=0, index=active_beam_idx)\n",
    "                enc_outputs  = torch.index_select(enc_outputs, dim=0, index=active_beam_idx)\n",
    "\n",
    "                emb1 = torch.index_select(emb1, dim=0, index=active_beam_idx)\n",
    "\n",
    "                blactions = torch.index_select(blactions, dim=0, index=active_beam_idx)\n",
    "                bllog_probs = torch.index_select(bllog_probs, dim=0, index=active_beam_idx)\n",
    "                sum_log_probs = torch.index_select(sum_log_probs, dim=0, index=active_beam_idx)\n",
    "\n",
    "            if done: break\n",
    "            decoder_input = enc_outputs[torch.arange(0, bpres_actions.shape[0], device=self.device), bpres_actions]\n",
    "\n",
    "            bdyn_inputs = self.dyn_feat.make_dynamic_feat(inputs.expand(beam_idx.shape[0], -1, -1), bpresent_time, bpres_actions, dist_mat, active_beam_idx)\n",
    "            emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "            enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "            policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs, adj_mask, decoder_input, dec_hidden, mask)\n",
    "\n",
    "            future_actions, log_probs, beam_idx = self.select_actions(policy, sum_log_probs, mask, infer_type)\n",
    "\n",
    "            # info update\n",
    "            h_step = torch.index_select(dec_hidden[0], dim=0, index = beam_idx)\n",
    "            c_step = torch.index_select(dec_hidden[1], dim=0, index = beam_idx)\n",
    "            dec_hidden = (h_step,c_step)\n",
    "\n",
    "            mask = torch.index_select(mask, dim=0, index=beam_idx)\n",
    "            adj_mask = torch.index_select(adj_mask, dim=0, index=beam_idx)\n",
    "\n",
    "            bpresent_time = torch.index_select(bpresent_time, dim=0, index=beam_idx)\n",
    "            bpres_actions = torch.index_select(bpres_actions, dim=0, index=beam_idx)\n",
    "\n",
    "            enc_outputs  = torch.index_select(enc_outputs, dim=0, index=beam_idx)\n",
    "\n",
    "            emb1 = torch.index_select(emb1, dim=0, index=beam_idx)\n",
    "\n",
    "            blactions = torch.index_select(blactions, dim=0, index=beam_idx)\n",
    "            bllog_probs = torch.index_select(bllog_probs, dim=0, index=beam_idx)\n",
    "            sum_log_probs = torch.index_select(sum_log_probs, dim=0, index=beam_idx)\n",
    "\n",
    "            sum_log_probs += log_probs.squeeze(0).detach()\n",
    "\n",
    "        return torch.cat(final_actions, dim=0), torch.cat(final_log_probs, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    def select_actions(self, policy, sum_log_probs, mask, infer_type = 'stochastic'):\n",
    "\n",
    "        beam_size, seq_size = policy.size()\n",
    "        nzn  = torch.nonzero(mask, as_tuple=False).shape[0]\n",
    "        sample_size = min(nzn,self.beam_size)\n",
    "\n",
    "        ourlogzero = sys.float_info.min\n",
    "        lpolicy = policy.masked_fill(mask == 0, ourlogzero).log()\n",
    "        npolicy = sum_log_probs.unsqueeze(1) + lpolicy\n",
    "        if infer_type == 'stochastic':\n",
    "            nnpolicy = npolicy.exp().masked_fill(mask == 0, 0).view(1, -1)\n",
    "\n",
    "            m = Categorical(nnpolicy)\n",
    "            gact_ind = torch.multinomial(nnpolicy, sample_size)\n",
    "            log_select =  m.log_prob(gact_ind)\n",
    "\n",
    "        elif infer_type == 'greedy':\n",
    "            nnpolicy = npolicy.exp().masked_fill(mask == 0, 0).view(1, -1)\n",
    "\n",
    "            _ , gact_ind = nnpolicy.topk(sample_size, dim = 1)\n",
    "            prob = policy.view(-1)[gact_ind]\n",
    "            log_select =  prob.log()\n",
    "\n",
    "        beam_id = torch.floor_divide(gact_ind, seq_size).squeeze(0)\n",
    "        act_ind = torch.fmod(gact_ind, seq_size)\n",
    "\n",
    "        return act_ind, log_select, beam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunEpisode(nn.Module):\n",
    "\n",
    "    def __init__(self, neuralnet, args):\n",
    "        super(RunEpisode, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.neuralnet = neuralnet\n",
    "        self.dyn_feat = DynamicFeatures(args)\n",
    "        self.lookahead = Lookahead(args)\n",
    "        self.mu = ModelUtils(args)\n",
    "\n",
    "    def forward(self, binputs, bdata_scaled, start_time, dist_mat, infer_type):\n",
    "\n",
    "        self.batch_size, sequence_size, input_size = binputs.size()\n",
    "\n",
    "        h_0, c_0 = self.neuralnet.decoder.hidden_0\n",
    "\n",
    "        dec_hidden = (h_0.expand(self.batch_size, -1), c_0.expand(self.batch_size, -1))\n",
    "\n",
    "        mask = torch.ones(self.batch_size, sequence_size, device=self.device, requires_grad=False, dtype = torch.uint8)\n",
    "\n",
    "        bpresent_time = start_time*torch.ones(self.batch_size, 1, device=self.device)\n",
    "\n",
    "        llog_probs, lactions, lstep_mask, lentropy = [], [], [], []\n",
    "\n",
    "        bpres_actions = torch.zeros(self.batch_size, dtype=torch.int64, device=self.device)\n",
    "\n",
    "        batch_idx = torch.arange(0, self.batch_size, device=self.device)\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat, bpres_actions,\n",
    "                                                 bpresent_time, batch_idx, first_step=True)\n",
    "\n",
    "        adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        # encoder first forward pass\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time, bpres_actions, dist_mat, batch_idx)\n",
    "        emb1 = self.neuralnet.sta_emb(bdata_scaled)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "        _, _, enc_outputs = self.neuralnet(enc_inputs, enc_inputs, adj_mask, enc_inputs, dec_hidden, mask, first_step=True)\n",
    "\n",
    "        decoder_input = enc_outputs[batch_idx, bpres_actions]\n",
    "\n",
    "        done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time, batch_idx)\n",
    "        adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask, dist_mat, bpres_actions, bpresent_time)\n",
    "\n",
    "        # encoder/decoder forward pass\n",
    "        bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time,\n",
    "                                                      bpres_actions, dist_mat, batch_idx)\n",
    "        emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "        enc_inputs = torch.cat((emb1,emb2), dim=2)\n",
    "\n",
    "        policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs, adj_mask, decoder_input, dec_hidden, mask)\n",
    "\n",
    "        lactions.append(bpres_actions)\n",
    "\n",
    "        # Starting the trip\n",
    "        while not done:\n",
    "\n",
    "            future_actions, log_probs, entropy = self.select_actions(policy, infer_type)\n",
    "\n",
    "            bpres_actions, bpresent_time, bstep_mask = self.mu.one_step_update(binputs, dist_mat, bpres_actions[batch_idx],\n",
    "                                                                               future_actions, bpresent_time[batch_idx],\n",
    "                                                                               batch_idx, self.batch_size)\n",
    "\n",
    "            blog_probs = torch.zeros(self.batch_size, 1, dtype=torch.float32).to(self.device)\n",
    "            blog_probs[batch_idx] = log_probs.unsqueeze(1)\n",
    "\n",
    "            bentropy = torch.zeros(self.batch_size,1,dtype=torch.float32).to(self.device)\n",
    "            bentropy[batch_idx] = entropy.unsqueeze(1)\n",
    "\n",
    "            llog_probs.append(blog_probs)\n",
    "            lactions.append(bpres_actions)\n",
    "            lstep_mask.append(bstep_mask)\n",
    "            lentropy.append(bentropy)\n",
    "\n",
    "            done, mask = self.mu.feasibility_control(binputs[batch_idx], mask, dist_mat,\n",
    "                                                     bpres_actions[batch_idx], bpresent_time[batch_idx],\n",
    "                                                     batch_idx)\n",
    "\n",
    "            if done: break\n",
    "            sub_batch_idx = torch.nonzero(mask[batch_idx][:,-1], as_tuple=False).squeeze(1)\n",
    "\n",
    "            batch_idx = torch.nonzero(mask[:,-1], as_tuple=False).squeeze(1)\n",
    "\n",
    "            adj_mask = self.lookahead.adjacency_matrix(binputs[batch_idx], mask[batch_idx], dist_mat, bpres_actions[batch_idx], bpresent_time[batch_idx])\n",
    "\n",
    "            #update decoder input and hidden\n",
    "            decoder_input = enc_outputs[sub_batch_idx, bpres_actions[sub_batch_idx]]\n",
    "            dec_hidden = (dec_hidden[0][sub_batch_idx], dec_hidden[1][sub_batch_idx])\n",
    "\n",
    "            # encoder/decoder forward pass\n",
    "            bdyn_inputs = self.dyn_feat.make_dynamic_feat(binputs, bpresent_time[batch_idx], bpres_actions[batch_idx], dist_mat, batch_idx)\n",
    "            emb2 = self.neuralnet.dyn_emb(bdyn_inputs)\n",
    "            enc_inputs = torch.cat((emb1[batch_idx],emb2), dim=2)\n",
    "\n",
    "            policy, dec_hidden, enc_outputs = self.neuralnet(enc_inputs, enc_outputs[sub_batch_idx], adj_mask, decoder_input, dec_hidden, mask[batch_idx])\n",
    "\n",
    "        return lactions, torch.cat(llog_probs, dim=1), torch.cat(lentropy, dim=1), torch.cat(lstep_mask, dim=1)\n",
    "\n",
    "\n",
    "    def select_actions(self, policy, infer_type):\n",
    "\n",
    "        if infer_type == 'stochastic':\n",
    "            m = Categorical(policy)\n",
    "            act_ind = m.sample()\n",
    "            log_select =  m.log_prob(act_ind)\n",
    "            poli_entro = m.entropy()\n",
    "        elif infer_type == 'greedy':\n",
    "            prob, act_ind = torch.max(policy, 1)\n",
    "            log_select =  prob.log()\n",
    "            poli_entro =  torch.zeros(self.batch_size, requires_grad=False).to(self.device)\n",
    "\n",
    "        return act_ind, log_select, poli_entro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py in the original code\n",
    "cf = dict(\n",
    "        BENCHMARK_INSTANCES_PATH = './data/benchmark/',\n",
    "        GENERATED_INSTANCES_PATH = './data/generated/',\n",
    "        RESULTS_PATH = './results'\n",
    ")\n",
    "cf = DotMap(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_config.py in the original code\n",
    "pcf = DotMap(dict(\n",
    "# Indices of instance data\n",
    "X_COORDINATE_IDX = 0,\n",
    "Y_COORDINATE_IDX = 1,\n",
    "VIS_DURATION_TIME_IDX = 2,\n",
    "OPENING_TIME_WINDOW_IDX = 3,\n",
    "CLOSING_TIME_WINDOW_IDX = 4,\n",
    "REWARD_IDX = 5,\n",
    "ARRIVAL_TIME_IDX = 6,\n",
    "\n",
    "# For generating instances\n",
    "SAMP_DAY_FRAC_INF = 4/24,\n",
    "UB_T_INIT_FRAC = 15/24,\n",
    "LB_T_MAX_FRAC = 12/24,\n",
    "CORR_SCORE_STD = 10,\n",
    "\n",
    "MULTIPLE_SCORE = 1.1,\n",
    "\n",
    "X_MAX = 100. # max square length (X_MAX)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_instance_data(instance_name, path):\n",
    "\n",
    "    \"\"\"reads instance data\"\"\"\n",
    "    PATH_TO_BENCHMARK_INSTANCES = path\n",
    "\n",
    "    benchmark_file = '{path_to_benchmark_instances}/{instance}.txt' \\\n",
    "                     .format(path_to_benchmark_instances=PATH_TO_BENCHMARK_INSTANCES,\n",
    "                             instance=instance_name)\n",
    "\n",
    "    dfile = open(benchmark_file)\n",
    "    data = [[float(x) for x in line.split()] for line in dfile]\n",
    "    dfile.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_extra_cordeau_columns(instance_data):\n",
    "    \"\"\"Cordeau instances have extra columns in some rows. This function eliminates the extra columns.\n",
    "    This will also correct position of total time in row 0 for all instances\"\"\"\n",
    "    DATA_INIT_ROW = 2\n",
    "    N_RELEVANT_FIRST_COLUMNS = 8\n",
    "    N_RELEVANT_LAST_COLUMNS = 2\n",
    "\n",
    "    return [s[:N_RELEVANT_FIRST_COLUMNS]+s[-N_RELEVANT_LAST_COLUMNS:] \\\n",
    "            for s in instance_data[DATA_INIT_ROW :]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instance_data(instance_data):\n",
    "    \"\"\"parse instance data into dataframe\"\"\"\n",
    "\n",
    "    OPENING_TIME_WINDOW_ABBREV_KEY = 'O'\n",
    "    CLOSING_TIME_WINDOW_ABBREV_KEY = 'C'\n",
    "    TOTAL_TIME_KEY = 'Total Time'\n",
    "    COLUMN_NAMES_ABBREV = ['i', 'x', 'y', 'd', 'S', 'f', 'a', 'list', 'O', 'C']\n",
    "\n",
    "    instance_data_clean = eliminate_extra_cordeau_columns(instance_data)\n",
    "    df = pd.DataFrame(instance_data_clean, columns=COLUMN_NAMES_ABBREV)\n",
    "\n",
    "    #add total time\n",
    "    df[TOTAL_TIME_KEY] = 0\n",
    "    df[TOTAL_TIME_KEY] = df.loc[0][CLOSING_TIME_WINDOW_ABBREV_KEY]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_n_vert_1(instance_data, instance_type):\n",
    "    N_VERT_ROW = 0\n",
    "\n",
    "    if instance_type=='Gavalas':\n",
    "        N_VERT_COL = 3\n",
    "        DATA_INIT_ROW = 1\n",
    "    else:\n",
    "        N_VERT_COL = 2\n",
    "        DATA_INIT_ROW = 2\n",
    "\n",
    "    n_vert = instance_data[N_VERT_ROW][N_VERT_COL]\n",
    "    count_vert = len(instance_data)-(DATA_INIT_ROW+1)\n",
    "\n",
    "    assert count_vert==n_vert, 'number of vertices doesnt match number of data rows'\n",
    "\n",
    "\n",
    "def test_n_vert_2(instance_data, instance_type):\n",
    "    N_VERT_ROW = 0\n",
    "    if instance_type=='Gavalas':\n",
    "        N_VERT_COL = 3\n",
    "    else:\n",
    "        N_VERT_COL = 2\n",
    "    COLUMN_NAMES = ['vertex number', 'x coordinate', 'y coordinate',\n",
    "                    'service duration or visiting time', 'profit of the location',\n",
    "                    'not relevant 1', 'not relevant 2', 'not relevant 3',\n",
    "                    'opening of time window', 'closing of time window']\n",
    "    COLUMN_NAMES = [s.replace(' ', '_') for s in COLUMN_NAMES]\n",
    "\n",
    "    VERTEX_NUMBER_COL = [i for i,n in enumerate(COLUMN_NAMES) if n=='vertex_number'][0]\n",
    "    n_vert = instance_data[N_VERT_ROW][N_VERT_COL]\n",
    "    last_vert_number = instance_data[-1][VERTEX_NUMBER_COL]\n",
    "\n",
    "    assert last_vert_number==n_vert, 'number of vertices doesnt match vertice count of last row'\n",
    "\n",
    "\n",
    "def test_n_vert_3(instance_data, instance_type):\n",
    "    if instance_type=='Gavalas':\n",
    "        N_DAYS_INDEX = 1\n",
    "        n_days = int(np.array(instance_data[0])[N_DAYS_INDEX])\n",
    "        assert n_days==1, 'not a single tour/1 day instance'\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(instance_df, instance_type):\n",
    "    \"\"\"\n",
    "    Distances between locations were rounded down to the first decimal\n",
    "    for the Solomon instances and to the second decimal for the instances of Cordeau and Gavalas.\n",
    "    \"\"\"\n",
    "\n",
    "    if instance_type in ['Solomon']:\n",
    "        n_digits = 10.0\n",
    "\n",
    "    elif instance_type in ['Cordeau', 'Gavalas']:\n",
    "        n_digits = 100.0\n",
    "\n",
    "    n = instance_df.shape[0]\n",
    "    distm = np.zeros((n,n))\n",
    "    x = instance_df.x.values\n",
    "    y = instance_df.y.values\n",
    "\n",
    "    for i in range(0, n-1):\n",
    "        for j in range(i+1, n):\n",
    "            distm[i,j] = np.floor(n_digits*(np.sqrt((x[i]-x[j])**2+(y[i]-y[j])**2)))/n_digits\n",
    "            distm[j,i] = distm[i,j]\n",
    "\n",
    "    return distm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_df(instance_name, path, instance_type):\n",
    "\n",
    "    \"\"\"combine read instance, tests and parse to dataframe\"\"\"\n",
    "\n",
    "    OPENING_TIME_WINDOW_ABBREV_KEY = 'O'\n",
    "    CLOSING_TIME_WINDOW_ABBREV_KEY = 'C'\n",
    "    TOTAL_TIME_KEY = 'Total Time'\n",
    "\n",
    "\n",
    "    COLUMN_NAMES = ['vertex number', 'x coordinate', 'y coordinate',\n",
    "    'service duration or visiting time', 'profit of the location',\n",
    "    'not relevant 1', 'not relevant 2', 'not relevant 3',\n",
    "    'opening of time window', 'closing of time window']\n",
    "    COLUMN_NAMES = [s.replace(' ', '_') for s in COLUMN_NAMES]\n",
    "    COLUMN_NAMES_ABBREV = ['i', 'x', 'y', 'd', 'S', 'f', 'a', 'list', 'O', 'C']\n",
    "    VERTEX_NUMBER_COL = [i for i,n in enumerate(COLUMN_NAMES) if n=='vertex_number'][0]\n",
    "    COLS_OF_INT = ['i', 'x', 'y', 'd', OPENING_TIME_WINDOW_ABBREV_KEY,\n",
    "                   CLOSING_TIME_WINDOW_ABBREV_KEY, 'S', TOTAL_TIME_KEY]\n",
    "    COLS_OF_INT_NEW_NAMES = ['i', 'x', 'y', 'duration', 'ti', 'tf', 'prof', TOTAL_TIME_KEY]\n",
    "\n",
    "    standard2newnames_dict =  dict(((c, ca) for c, ca in zip(COLS_OF_INT, COLS_OF_INT_NEW_NAMES)))\n",
    "\n",
    "    instance_data = read_instance_data(instance_name, path)\n",
    "\n",
    "    # run tests\n",
    "    test_n_vert_1(instance_data, instance_type)\n",
    "    test_n_vert_2(instance_data, instance_type)\n",
    "    # test if it's a single day (we are not considering TOPTW instances)\n",
    "    test_n_vert_3(instance_data, instance_type)\n",
    "\n",
    "    if instance_type=='Gavalas':\n",
    "        df = parse_instance_data_Gavalas(instance_data)\n",
    "    else:\n",
    "        df = parse_instance_data(instance_data)\n",
    "\n",
    "    #change column names\n",
    "    COLS_OF_INT_NEW_NAMES = [standard2newnames_dict[s] for s in COLS_OF_INT]\n",
    "    df_ = df[COLS_OF_INT].copy()\n",
    "    df_.columns = COLS_OF_INT_NEW_NAMES\n",
    "    df_['inst_name'] = instance_name\n",
    "    df_['real_or_val'] = 'real'\n",
    "\n",
    "    df_ = df_.append(df_.loc[0])\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_data(args, phase='train'):\n",
    "\n",
    "    df = get_instance_df(args.instance, cf.BENCHMARK_INSTANCES_PATH,\n",
    "                         args.instance_type)\n",
    "    dist_mat = get_distance_matrix(df, args.instance_type)\n",
    "    inp_real = df[['x', 'y', 'duration', 'ti', 'tf', 'prof', 'Total Time']].values\n",
    "\n",
    "    if phase=='train':\n",
    "        inp_real = [(torch.FloatTensor(inp_real).to(args.device),\n",
    "                torch.tensor(inp_real[0, pcf.OPENING_TIME_WINDOW_IDX]).to(args.device),\n",
    "                torch.FloatTensor(dist_mat).to(args.device))]\n",
    "\n",
    "        new_inp_real = [(args.instance, inp_real[0])]\n",
    "        return new_inp_real\n",
    "    else:\n",
    "        inp_real = [(torch.FloatTensor(inp_real).to(args.device),\n",
    "                 torch.FloatTensor(dist_mat).to(args.device))]\n",
    "        return inp_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Norm Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_dependent_norm_const(instance_raw_data):\n",
    "    day_duration = int(instance_raw_data[:, pcf.CLOSING_TIME_WINDOW_IDX].max().item())\n",
    "    t_max_real = int(instance_raw_data[0, pcf.ARRIVAL_TIME_IDX].item()) # max instance arrival time\n",
    "    arrival_time_val_ub = t_max_real+int(pcf.SAMP_DAY_FRAC_INF*day_duration)\n",
    "    Tmax = int(max(day_duration, arrival_time_val_ub)) # max possible time value\n",
    "    Smax = int(torch.round(pcf.MULTIPLE_SCORE*instance_raw_data[1:-1, pcf.REWARD_IDX].max()).item()) # max score\n",
    "\n",
    "    return Tmax, Smax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaler(data, norm_dic):\n",
    "    datan = data.clone()\n",
    "    datan[:, pcf.X_COORDINATE_IDX] /= pcf.X_MAX\n",
    "    datan[:, pcf.Y_COORDINATE_IDX] /= pcf.X_MAX\n",
    "    datan[:, pcf.VIS_DURATION_TIME_IDX] /= (datan[:, pcf.VIS_DURATION_TIME_IDX].max())\n",
    "    datan[:, pcf.OPENING_TIME_WINDOW_IDX] /= norm_dic['Tmax']\n",
    "    datan[:, pcf.CLOSING_TIME_WINDOW_IDX ] /= norm_dic['Tmax']\n",
    "    datan[:, pcf.REWARD_IDX] /= norm_dic['Smax']\n",
    "    datan[:, pcf.ARRIVAL_TIME_IDX] /= norm_dic['Tmax']\n",
    "\n",
    "    return datan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(data, sample_solution, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        Tensor of shape [batch_size] containing rewards\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = sample_solution[0].shape[0]\n",
    "    tour_reward = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    for act_id in sample_solution:\n",
    "        tour_reward += data[act_id, pcf.REWARD_IDX].squeeze(0)\n",
    "\n",
    "    return tour_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single(inst_data, norm_dic, start_time, dist_mat, args, model,\n",
    "               which_inf=None):\n",
    "\n",
    "\n",
    "    saved_model_path = args.load_w_dir +'/model_' + str(args.saved_model_epoch) + '.pkl'\n",
    "    model._load_model_weights(saved_model_path, args.device)\n",
    "\n",
    "\n",
    "    tic = time.time()\n",
    "    if which_inf=='bs':\n",
    "        run_episode_inf = BeamSearch(model, args).eval()\n",
    "        route, score = bs_inference(inst_data, norm_dic, start_time, dist_mat,\n",
    "                                    args, run_episode_inf)\n",
    "\n",
    "    elif which_inf=='gr':\n",
    "        run_episode_inf = RunEpisode(model, args).eval()\n",
    "        route, score = gr_inference(inst_data, norm_dic, start_time, dist_mat,\n",
    "                                    args, run_episode_inf)\n",
    "\n",
    "    elif which_inf=='as_bs':\n",
    "\n",
    "        saved_model_path = args.load_w_dir +'/model_' + str(args.saved_model_epoch) + '.pkl'\n",
    "        model._load_model_weights(saved_model_path, args.device)\n",
    "        run_episode_train = RunEpisode(model, args)\n",
    "\n",
    "        run_episode_inf = BeamSearch(model, args).eval()\n",
    "        inp_data = (inst_data, start_time, dist_mat)\n",
    "        route, score = as_bs_inference(inp_data, norm_dic, args,\n",
    "                                       run_episode_train, run_episode_inf)\n",
    "    toc = time.time()\n",
    "\n",
    "    output = dict([('score', score), ('route', route), ('inf_time', toc-tic)])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_inference(inst_data, norm_dic, start_time, dist_mat, args, run_episode):\n",
    "\n",
    "    data_scaled = data_scaler(inst_data, norm_dic)\n",
    "    binst_data, bdata_scaled = inst_data.unsqueeze(0), data_scaled.unsqueeze(0)\n",
    "\n",
    "    nb = args.max_beam_number\n",
    "    with torch.no_grad():\n",
    "        seq, _ = run_episode(binst_data, bdata_scaled, start_time, dist_mat, 'greedy', nb)\n",
    "\n",
    "    seq_list = [ seq[:,k] for k in range(seq.shape[1])]\n",
    "    rewards = reward_fn(inst_data, seq_list, args.device)\n",
    "    maxrew, idx_max = torch.max(rewards, 0)\n",
    "    score = maxrew.item()\n",
    "\n",
    "    route =  [0] + [val.item() for val in seq[idx_max] if val.item() != 0]\n",
    "    route[-1] = 0\n",
    "    return route, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicFeatures():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(DynamicFeatures, self).__init__()\n",
    "\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.device = args.device\n",
    "\n",
    "    def make_dynamic_feat(self, data, current_time, current_poi_idx, dist_mat, batch_idx):\n",
    "\n",
    "        num_dyn_feat = 8\n",
    "        _ , sequence_size, input_size  = data.size()\n",
    "        batch_size = batch_idx.shape[0]\n",
    "\n",
    "        dyn_feat = torch.ones(batch_size, sequence_size, num_dyn_feat).to(self.device)\n",
    "\n",
    "        tour_start_time = data[0, 0, self.opening_time_window_idx]\n",
    "        max_tour_duration = data[0, 0, self.arrival_time_idx] - tour_start_time\n",
    "        arrive_j_times = current_time + dist_mat[current_poi_idx]\n",
    "\n",
    "        dyn_feat[:, :, 0] = (data[batch_idx, :, self.opening_time_window_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 1] = (data[batch_idx, :, self.closing_time_window_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 2] = (data[batch_idx, :, self.arrival_time_idx] - current_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 3] = (current_time - tour_start_time) / max_tour_duration\n",
    "\n",
    "\n",
    "        dyn_feat[:, :, 4] = (arrive_j_times - tour_start_time) / max_tour_duration\n",
    "        dyn_feat[:, :, 5] = (data[batch_idx, :, self.opening_time_window_idx] - arrive_j_times) / max_tour_duration\n",
    "        dyn_feat[:, :, 6] = (data[batch_idx, :, self.closing_time_window_idx] - arrive_j_times) / max_tour_duration\n",
    "        dyn_feat[:, :, 7] = (data[batch_idx, :, self.arrival_time_idx] - arrive_j_times) / max_tour_duration\n",
    "\n",
    "        return dyn_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead():\n",
    "    def __init__(self, args):\n",
    "        super(Lookahead, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.vis_duration_time_idx = pcf.VIS_DURATION_TIME_IDX\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "\n",
    "    def adjacency_matrix(self, braw_inputs, mask, dist_mat, pres_act, present_time):\n",
    "        # feasible neighborhood for each node\n",
    "        maskk = mask.clone()\n",
    "        step_batch_size, npoints = mask.shape\n",
    "\n",
    "        #one step forward update\n",
    "        arrivej = dist_mat[pres_act] + present_time\n",
    "        farrivej = arrivej.view(step_batch_size, npoints)\n",
    "        tw_start = braw_inputs[:, :, self.opening_time_window_idx]\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), tw_start-farrivej)\n",
    "        durat = braw_inputs[:, : , self.vis_duration_time_idx]\n",
    "\n",
    "        fpresent_time = farrivej + waitj + durat\n",
    "        fpres_act = torch.arange(0, npoints, device=self.device).expand(step_batch_size, -1)\n",
    "\n",
    "        # feasible neighborhood for each node\n",
    "        adj_mask = maskk.unsqueeze(1).repeat(1, npoints, 1)\n",
    "        arrivej = dist_mat.expand(step_batch_size, -1, -1) + fpresent_time.unsqueeze(2)\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), tw_start.unsqueeze(2)-arrivej)\n",
    "\n",
    "        tw_end = braw_inputs[:, :, self.closing_time_window_idx]\n",
    "        ttime = braw_inputs[:, 0, self.arrival_time_idx]\n",
    "\n",
    "        dlast = dist_mat[:, -1].unsqueeze(0).expand(step_batch_size, -1)\n",
    "\n",
    "        c1 = arrivej + waitj <= tw_end.unsqueeze(1)\n",
    "        c2 = arrivej + waitj + durat.unsqueeze(1) + dlast.unsqueeze(1) <= ttime.unsqueeze(1).unsqueeze(1).expand(-1, npoints, npoints)\n",
    "        adj_mask = adj_mask * c1 * c2\n",
    "\n",
    "        # self-loop\n",
    "        idx = torch.arange(0, npoints, device=self.device).expand(step_batch_size, -1)\n",
    "        adj_mask[:, idx, idx] = 1\n",
    "\n",
    "        return adj_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelUtils():\n",
    "    def __init__(self, args):\n",
    "        super(ModelUtils, self).__init__()\n",
    "\n",
    "        self.device = args.device\n",
    "        self.opening_time_window_idx = pcf.OPENING_TIME_WINDOW_IDX\n",
    "        self.closing_time_window_idx = pcf.CLOSING_TIME_WINDOW_IDX\n",
    "        self.vis_duration_time_idx = pcf.VIS_DURATION_TIME_IDX\n",
    "        self.arrival_time_idx = pcf.ARRIVAL_TIME_IDX\n",
    "\n",
    "    def feasibility_control(self, braw_inputs, mask, dist_mat, pres_act, present_time, batch_idx, first_step=False):\n",
    "\n",
    "        done = False\n",
    "        maskk = mask.clone()\n",
    "        step_batch_size = batch_idx.shape[0]\n",
    "\n",
    "        arrivej = dist_mat[pres_act] + present_time\n",
    "        waitj = torch.max(torch.FloatTensor([0.0]).to(self.device), braw_inputs[:, :, self.opening_time_window_idx]-arrivej)\n",
    "\n",
    "        c1 = arrivej + waitj <= braw_inputs[:, :, self.closing_time_window_idx]\n",
    "        c2 = arrivej + waitj + braw_inputs[:, :, self.vis_duration_time_idx] + dist_mat[:, -1] <= braw_inputs[0, 0, self.arrival_time_idx]\n",
    "\n",
    "        if not first_step:\n",
    "            maskk[batch_idx, pres_act] = 0\n",
    "\n",
    "        maskk[batch_idx] = maskk[batch_idx] * c1 * c2\n",
    "\n",
    "        if maskk[:, -1].any() == 0:\n",
    "            done = True\n",
    "        return done, maskk\n",
    "\n",
    "    \n",
    "    def one_step_update(self, raw_inputs_b, dist_mat, pres_action, future_action, present_time, batch_idx, batch_size):\n",
    "\n",
    "        present_time_b = torch.zeros(batch_size, 1, device=self.device)\n",
    "        pres_actions_b = torch.zeros(batch_size, dtype=torch.int64, device=self.device)\n",
    "        step_mask_b = torch.zeros(batch_size, 1, device=self.device, requires_grad=False, dtype=torch.bool)\n",
    "\n",
    "        arrive_j = dist_mat[pres_action, future_action].unsqueeze(1) + present_time\n",
    "        wait_j = torch.max(torch.FloatTensor([0.0]).to(self.device),\n",
    "                           raw_inputs_b[batch_idx, future_action, self.opening_time_window_idx].unsqueeze(1)-arrive_j)\n",
    "        present_time = arrive_j + wait_j + raw_inputs_b[batch_idx, future_action, self.vis_duration_time_idx].unsqueeze(1)\n",
    "\n",
    "        present_time_b[batch_idx] = present_time\n",
    "\n",
    "        pres_actions_b[batch_idx] = future_action\n",
    "        step_mask_b[batch_idx] = 1\n",
    "\n",
    "        return pres_actions_b, present_time_b, step_mask_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    batch_size=32, \n",
    "    beta=0.01, \n",
    "    device='cpu', \n",
    "    debug=False, \n",
    "    device_name='cpu', \n",
    "    ff_dim=256, \n",
    "    generated=False, \n",
    "    infe_type='bs', \n",
    "    instance='pr01', \n",
    "    instance_type='Cordeau', \n",
    "    load_w_dir='./results/pr01/model_w/model_article_uni_samp', \n",
    "    lr=1e-05, \n",
    "    map_location={'cpu': 'cpu'}, \n",
    "    max_beam_number=128, \n",
    "    max_grad_norm=1, \n",
    "    model_name='article', \n",
    "    n_heads=8, \n",
    "    n_layers=2, \n",
    "    ndfeatures=8, \n",
    "    nepocs=128, \n",
    "    nfeatures=7, \n",
    "    nprint=1, \n",
    "    rnn_hidden=128, \n",
    "    sample_type='uni_samp', \n",
    "    saved_model_epoch=500000, \n",
    "    seed=2925, \n",
    "    use_checkpoint=False, \n",
    "    val_dir='./data/generated//pr01', val_set_pt_file='inp_val_uni_samp.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DotMap(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_real = get_real_data(args, phase='inference')\n",
    "raw_data, raw_distm = inp_real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = raw_data[0, pcf.OPENING_TIME_WINDOW_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Tmax and Smax\n",
    "norm_dic = {}\n",
    "Tmax, Smax = instance_dependent_norm_const(raw_data)\n",
    "norm_dic = {'Tmax': Tmax, 'Smax': Smax}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_scores = []\n",
    "\n",
    "# args.nfeatures : Number of Static features\n",
    "# args.ndfeatures: Number of Dynamic features\n",
    "# args.rnn_hidden: Number of Hidden features \n",
    "\n",
    "pointer_net = RecPointerNetwork(args.nfeatures, args.ndfeatures,\n",
    "                          args.rnn_hidden, args).to(args.device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering route for benchmark instance...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# inference\n",
    "# ---------------------------------\n",
    "\n",
    "if not args.generated:\n",
    "    print('Infering route for benchmark instance...')\n",
    "    output =  run_single(raw_data, norm_dic, start_time, raw_distm, args,\n",
    "                            pointer_net, which_inf=args.infe_type)\n",
    "\n",
    "else:\n",
    "    inp_val = u.get_val_data(args, phase='inference')\n",
    "    print('Infering routes for {num_inst} generated instances...' \\\n",
    "                .format(num_inst=len(inp_val)))\n",
    "    outputs =  iu.run_multiple(inp_val, norm_dic, args, pointer_net,\n",
    "                               which_inf=args.infe_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "route: [0, 9, 24, 47, 12, 38, 30, 2, 32, 37, 10, 45, 11, 28, 1, 16, 36, 31, 35, 34, 22, 7, 0]\n",
      "total score: 308\n",
      "inference time: 9518 ms\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Log results\n",
    "# ---------------------------------\n",
    "N_DASHES = 40\n",
    "if args.infe_type in ['gr', 'bs', 'as_bs']:\n",
    "\n",
    "    print(N_DASHES*'-')\n",
    "    if not args.generated:\n",
    "        print('route: {route}'.format(route=output['route']))\n",
    "        print('total score: {total_score}'\\\n",
    "                    .format(total_score=int(output['score'])))\n",
    "        inference_time_ms = int(1000*output['inf_time'])\n",
    "        print('inference time: {inference_time} ms'\\\n",
    "                    .format(inference_time=inference_time_ms))\n",
    "\n",
    "    else:\n",
    "        df_out = pd.DataFrame(outputs)\n",
    "        average_total_score = round(df_out.score.mean(), 2)\n",
    "        average_inf_time_ms = int(1000*df_out.inf_time.mean())\n",
    "        print('average total score: {average_total_score}' \\\n",
    "                    .format(average_total_score=average_total_score))\n",
    "        print('average inference time: {average_inference_time} ms' \\\n",
    "                    .format(average_inference_time=average_inf_time_ms))\n",
    "    print(N_DASHES*'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:optw_env]",
   "language": "python",
   "name": "conda-env-optw_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
